# Prometheus Alerting Rules for FreeHekim RAG API
# Production monitoring alerts

groups:
  - name: rag_api_alerts
    interval: 30s
    rules:
      # API Health Check
      - alert: RagApiDown
        expr: up{job="rag-api"} == 0
        for: 2m
        labels:
          severity: critical
          component: api
        annotations:
          summary: "RAG API is down"
          description: "RAG API has been down for more than 2 minutes"

      # High Error Rate
      - alert: HighErrorRate
        expr: rate(rag_errors_total[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
          component: api
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} errors/sec for the last 5 minutes"

      # High Response Time
      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(rag_total_seconds_bucket[5m])) > 2.0
        for: 5m
        labels:
          severity: warning
          component: api
        annotations:
          summary: "High API response time"
          description: "95th percentile response time is {{ $value }}s"

      # High Memory Usage
      - alert: HighMemoryUsage
        expr: container_memory_usage_bytes{name="docker-api-1"} / container_spec_memory_limit_bytes{name="docker-api-1"} > 0.9
        for: 5m
        labels:
          severity: warning
          component: api
        annotations:
          summary: "High memory usage in API container"
          description: "Memory usage is at {{ $value | humanizePercentage }}"

      # High CPU Usage
      - alert: HighCPUUsage
        expr: rate(container_cpu_usage_seconds_total{name="docker-api-1"}[5m]) > 1.8
        for: 10m
        labels:
          severity: warning
          component: api
        annotations:
          summary: "High CPU usage in API container"
          description: "CPU usage is {{ $value }} cores"

  - name: qdrant_alerts
    interval: 30s
    rules:
      # Qdrant Health
      - alert: QdrantDown
        expr: up{job="qdrant"} == 0
        for: 2m
        labels:
          severity: critical
          component: qdrant
        annotations:
          summary: "Qdrant vector database is down"
          description: "Qdrant has been unreachable for more than 2 minutes"

  - name: prometheus_alerts
    interval: 30s
    rules:
      # Prometheus Self-monitoring
      - alert: PrometheusDown
        expr: up{job="prometheus"} == 0
        for: 5m
        labels:
          severity: critical
          component: prometheus
        annotations:
          summary: "Prometheus is down"
          description: "Prometheus monitoring is not scraping itself"
